# Home_Remedies_AI_VibhavRaj

# ğŸ  Home Remedies AI â€” TinyLlama + LangChain (FastAPI Dashboard)

A **FastAPI-powered AI web application** that answers questions about **natural home remedies** using the **TinyLlama** model through **LM Studio** and **LangChain**.  
This project connects a **local LLM (TinyLlama in LM Studio)** to a **custom FastAPI dashboard**, creating a seamless local AI experience ğŸ’¡.

---

# ğŸ  Home Remedies AI â€” TinyLlama + LangChain (FastAPI Dashboard)

[![Python](https://img.shields.io/badge/Python-3.10%2B-blue?logo=python)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-Framework-009688?logo=fastapi)](https://fastapi.tiangolo.com/)
[![LangChain](https://img.shields.io/badge/LangChain-Enabled-8A2BE2?logo=chainlink)](https://www.langchain.com/)
[![LM Studio](https://img.shields.io/badge/LM%20Studio-TinyLlama-orange?logo=openai)](https://lmstudio.ai)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellowgreen.svg)](LICENSE)
[![Status](https://img.shields.io/badge/Status-Active-brightgreen)](#)
[![Bootstrap](https://img.shields.io/badge/UI-Bootstrap%205-blueviolet?logo=bootstrap)](https://getbootstrap.com/)
[![Chart.js](https://img.shields.io/badge/Charts-Chart.js-red?logo=chartdotjs)](https://www.chartjs.org/)


## ğŸŒŸ Features

### ğŸ’¬ Ask About Any Home Remedy
- Example: _â€œHow can I cure cold naturally?â€_ ğŸŒ¿  
- Uses **TinyLlama** through **LM Studio** to generate accurate, context-based answers.
- References your own **PDF knowledge base** (`Basic_Home_Remedies.pdf`).

### ğŸ“Š Professional Dashboard
- ğŸ” **Admin login** with access token  
- ğŸ“ˆ **Live metrics** (total questions, errors, response time)  
- ğŸ—‚ï¸ **Recent queries table**  
- ğŸ”„ Clear logs and reload system cache

### ğŸ§  Intelligent Backend
- Uses **LangChain** to manage retrieval + generation.
- Embeds your PDF into a **vector store (FAISS)** for fast context-based retrieval.
- Works fully **offline** after LM Studio setup.

---

## ğŸ§© Project Structure

Home_Remedies_AI/
â”‚
â”œâ”€â”€ app.py # Main FastAPI app (Frontend + Dashboard)
â”œâ”€â”€ main.py # Optional runner (if needed)
â”œâ”€â”€ Basic_Home_Remedies.pdf # Knowledge base used for QA
â”œâ”€â”€ requirements.txt # Dependencies list
â”œâ”€â”€ README.md # Project documentation
â”‚
â”œâ”€â”€ qa_engine/ # Core AI logic (LangChain + LM Studio)
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ pdf_loader.py # Loads and splits PDF text
â”‚ â”œâ”€â”€ vector_store.py # Embedding + FAISS vector DB
â”‚ â””â”€â”€ qa.py / qa_core.py # Main Q&A pipeline
â”‚
â”œâ”€â”€ models/ # (Optional) model cache or embedding store
â”‚
â”œâ”€â”€ static/ # Frontend static assets
â”‚ â”œâ”€â”€ style.css
â”‚ â””â”€â”€ dashboard.js
â”‚
â”œâ”€â”€ templates/ # Frontend pages
â”‚ â”œâ”€â”€ index.html # User interface
â”‚ â”œâ”€â”€ result.html # AI answer page
â”‚ â””â”€â”€ dashboard.html # Admin dashboard
â”‚
â”œâ”€â”€ venv/ # Python virtual environment (ignored by git)
â””â”€â”€ pycache/ # Cached files (ignored)

markdown
Copy code

---

## ğŸ§  LM Studio Configuration & Setup

Youâ€™ve already set up **LM Studio**, and thatâ€™s the key engine behind this project.  
Hereâ€™s what you did (and others should do) to make it work:

### âš™ï¸ Steps You Did in LM Studio

1. **Installed LM Studio** â€” from [https://lmstudio.ai](https://lmstudio.ai)  
2. **Downloaded the TinyLlama Model**  
   - Example: `TinyLlama-1.1B-Chat-v1.0` or any compatible instruction-tuned model  
   - Loaded it in LM Studioâ€™s local model runner.  
3. **Started the Local Server (API Mode)**  
   - Open LM Studio â†’ Go to the **Server** tab  
   - Enabled **Local Inference Server**  
   - Confirm it runs on â†’ `http://127.0.0.1:1234/v1`  
4. **Verified it works**  
   - Open a browser and visit: [http://127.0.0.1:1234/v1/models](http://127.0.0.1:1234/v1/models)  
   - It should show your model name like `TinyLlama-1.1B-Chat`.

âœ… **This FastAPI project automatically connects to that LM Studio endpoint** using:
```python
os.environ["OPENAI_API_BASE"] = "http://127.0.0.1:1234/v1"
os.environ["OPENAI_API_KEY"] = "lm-studio"


ğŸ–¥ï¸ **How the System Works (Flow Diagram)**

sql
Copy code
          +---------------------------+
          |  Basic_Home_Remedies.pdf  |
          +-------------+-------------+
                        |
                        v
                [LangChain Loader]
                        |
                        v
                 [Vector Store - FAISS]
                        |
                        v
       +-------------------------------------+
       |   User Asks Question via Web App    |
       +-------------------------------------+
                        |
                        v
         [LangChain Retriever + TinyLlama]
                        |
                        v
           ğŸ’¡ AI Answer Displayed in UI



## âš™ï¸ Installation & Running the Project

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/yourusername/Home_Remedies_AI.git
cd Home_Remedies_AI



### 2ï¸âƒ£ Create Virtual Environment
python -m venv venv
venv\Scripts\activate   # On Windows
# OR
source venv/bin/activate   # On macOS/Linux


### 3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt



### 4ï¸âƒ£ Start LM Studio Server

**Make sure LM Studio is running locally on**
ğŸ‘‰ http://127.0.0.1:1234

with the TinyLlama model loaded.



ğŸ§  **Inside LM Studio:**

Open LM Studio

Go to the Models tab

Download and load TinyLlama-1.1B-Chat-v1.0

Start the Local Server

Confirm API live at http://127.0.0.1:1234/v1/models



### 5ï¸âƒ£ Run the FastAPI App
uvicorn app:app --reload


Visit:
ğŸŒ User Interface: http://127.0.0.1:8000

ğŸ§­ Admin Dashboard: http://127.0.0.1:8000/admin/dashboard

Default Admin Token: dev-token-please-change

ğŸ”‘ ### Environment Variables
Variable	Description	Default
OPENAI_API_BASE	LM Studio endpoint	http://127.0.0.1:1234/v1
OPENAI_API_KEY	Fake API key for LM Studio	lm-studio
HOME_REMEDIES_ADMIN_TOKEN	Dashboard access token	dev-token-please-change


ğŸ§ ### How It Works Internally

PDF Loading â€“ The app reads Basic_Home_Remedies.pdf and extracts text.

Embedding Creation â€“ LangChain converts each text chunk into numerical vectors.

Vector Search â€“ When you ask a question, it finds the most relevant chunks.

TinyLlama Response â€“ LM Studio (TinyLlama) generates a natural language answer.

Response Display â€“ The answer and metadata appear in the frontend.



ğŸ“Š ### API Endpoints
Endpoint	Method	Description
/	GET	Main user interface
/ask	POST	Ask a question via HTML form
/api/ask	POST	API endpoint for JSON queries
/health	GET	Health check
/admin/dashboard	GET	Dashboard page
/admin/api/metrics	GET	Metrics API
/admin/api/clear-history	POST	Clear logs


ğŸ§° ### Troubleshooting
Problem	Possible Solution
âŒ QA system not ready	Start LM Studio with TinyLlama loaded
âš ï¸ ModuleNotFoundError: qa_engine	Check folder structure & __init__.py
ğŸ”’ Dashboard unauthorized	Use correct admin token
ğŸ–¼ï¸ Static/Template not loading	Verify paths to static/ & templates/
ğŸ§‘â€ğŸ’» Tech Stack
Layer	Technology
Frontend	HTML + CSS + JS + Jinja2
Backend	FastAPI
AI Engine	TinyLlama (via LM Studio)
Framework	LangChain
Storage	FAISS (Vector Store)
Charts	Chart.js
UI Styling	Bootstrap 5


ğŸ“¸ Preview


ğŸ  User Interface


ğŸ“Š Admin Dashboard

ğŸš€ Future Enhancements

ğŸ”„ Live answer streaming via WebSocket

ğŸ’¾ Persistent metrics with SQLite

ğŸ§­ Multi-PDF knowledge base

ğŸŒ™ Dark mode for dashboard

ğŸ¤– Support for other local models in LM Studio

ğŸ§­ Multi-PDF knowledge base

ğŸŒ™ Dark mode for dashboard

ğŸ¤– Support for other local models in LM Studio
