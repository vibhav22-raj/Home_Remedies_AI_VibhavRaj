# Home_Remedies_AI_VibhavRaj

# ğŸ  Home Remedies AI â€” TinyLlama + LangChain (FastAPI Dashboard)

A **FastAPI-powered AI web application** that answers questions about **natural home remedies** using the **TinyLlama** model through **LM Studio** and **LangChain**.  
This project connects a **local LLM (TinyLlama in LM Studio)** to a **custom FastAPI dashboard**, creating a seamless local AI experience ğŸ’¡.

---

## ğŸŒŸ Features

### ğŸ’¬ Ask About Any Home Remedy
- Example: _â€œHow can I cure cold naturally?â€_ ğŸŒ¿  
- Uses **TinyLlama** through **LM Studio** to generate accurate, context-based answers.
- References your own **PDF knowledge base** (`Basic_Home_Remedies.pdf`).

### ğŸ“Š Professional Dashboard
- ğŸ” **Admin login** with access token  
- ğŸ“ˆ **Live metrics** (total questions, errors, response time)  
- ğŸ—‚ï¸ **Recent queries table**  
- ğŸ”„ Clear logs and reload system cache

### ğŸ§  Intelligent Backend
- Uses **LangChain** to manage retrieval + generation.
- Embeds your PDF into a **vector store (FAISS)** for fast context-based retrieval.
- Works fully **offline** after LM Studio setup.

---

## ğŸ§© Project Structure

Home_Remedies_AI/
â”‚
â”œâ”€â”€ app.py # Main FastAPI app (Frontend + Dashboard)
â”œâ”€â”€ main.py # Optional runner (if needed)
â”œâ”€â”€ Basic_Home_Remedies.pdf # Knowledge base used for QA
â”œâ”€â”€ requirements.txt # Dependencies list
â”œâ”€â”€ README.md # Project documentation
â”‚
â”œâ”€â”€ qa_engine/ # Core AI logic (LangChain + LM Studio)
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ pdf_loader.py # Loads and splits PDF text
â”‚ â”œâ”€â”€ vector_store.py # Embedding + FAISS vector DB
â”‚ â””â”€â”€ qa.py / qa_core.py # Main Q&A pipeline
â”‚
â”œâ”€â”€ models/ # (Optional) model cache or embedding store
â”‚
â”œâ”€â”€ static/ # Frontend static assets
â”‚ â”œâ”€â”€ style.css
â”‚ â””â”€â”€ dashboard.js
â”‚
â”œâ”€â”€ templates/ # Frontend pages
â”‚ â”œâ”€â”€ index.html # User interface
â”‚ â”œâ”€â”€ result.html # AI answer page
â”‚ â””â”€â”€ dashboard.html # Admin dashboard
â”‚
â”œâ”€â”€ venv/ # Python virtual environment (ignored by git)
â””â”€â”€ pycache/ # Cached files (ignored)

markdown
Copy code

---

## ğŸ§  LM Studio Configuration & Setup

Youâ€™ve already set up **LM Studio**, and thatâ€™s the key engine behind this project.  
Hereâ€™s what you did (and others should do) to make it work:

### âš™ï¸ Steps You Did in LM Studio

1. **Installed LM Studio** â€” from [https://lmstudio.ai](https://lmstudio.ai)  
2. **Downloaded the TinyLlama Model**  
   - Example: `TinyLlama-1.1B-Chat-v1.0` or any compatible instruction-tuned model  
   - Loaded it in LM Studioâ€™s local model runner.  
3. **Started the Local Server (API Mode)**  
   - Open LM Studio â†’ Go to the **Server** tab  
   - Enabled **Local Inference Server**  
   - Confirm it runs on â†’ `http://127.0.0.1:1234/v1`  
4. **Verified it works**  
   - Open a browser and visit: [http://127.0.0.1:1234/v1/models](http://127.0.0.1:1234/v1/models)  
   - It should show your model name like `TinyLlama-1.1B-Chat`.

âœ… **This FastAPI project automatically connects to that LM Studio endpoint** using:
```python
os.environ["OPENAI_API_BASE"] = "http://127.0.0.1:1234/v1"
os.environ["OPENAI_API_KEY"] = "lm-studio"
ğŸ–¥ï¸ How the System Works (Flow Diagram)
sql
Copy code
          +---------------------------+
          |  Basic_Home_Remedies.pdf  |
          +-------------+-------------+
                        |
                        v
                [LangChain Loader]
                        |
                        v
                 [Vector Store - FAISS]
                        |
                        v
       +-------------------------------------+
       |   User Asks Question via Web App    |
       +-------------------------------------+
                        |
                        v
         [LangChain Retriever + TinyLlama]
                        |
                        v
           ğŸ’¡ AI Answer Displayed in UI
âš™ï¸ Installation & Running the Project
1ï¸âƒ£ Clone the Repository
bash
Copy code
git clone https://github.com/yourusername/Home_Remedies_AI.git
cd Home_Remedies_AI
2ï¸âƒ£ Create Virtual Environment
bash
Copy code
python -m venv venv
venv\Scripts\activate   # On Windows
# OR
source venv/bin/activate   # On macOS/Linux
3ï¸âƒ£ Install Dependencies
bash
Copy code
pip install -r requirements.txt
4ï¸âƒ£ Start LM Studio Server
Make sure LM Studio is running locally on
ğŸ‘‰ http://127.0.0.1:1234
with the TinyLlama model loaded.

5ï¸âƒ£ Run the FastAPI App
bash
Copy code
uvicorn app:app --reload
Visit:

ğŸŒ User Interface: http://127.0.0.1:8000/

ğŸ§­ Admin Dashboard: http://127.0.0.1:8000/admin/dashboard

Default Admin Token: dev-token-please-change

ğŸ”‘ Environment Variables
Variable	Description	Default
OPENAI_API_BASE	LM Studio endpoint	http://127.0.0.1:1234/v1
OPENAI_API_KEY	Fake API key for LM Studio	lm-studio
HOME_REMEDIES_ADMIN_TOKEN	Dashboard access token	dev-token-please-change

ğŸ§  How It Works Internally
PDF Loading: The app reads Basic_Home_Remedies.pdf and extracts text.

Embedding Creation: LangChain converts each text chunk into numerical vectors.

Vector Search: When you ask a question, it finds the most relevant chunks.

TinyLlama Response: LM Studio (TinyLlama) generates a natural language answer.

Response Display: The answer and metadata appear in the frontend.

ğŸ“Š API Endpoints
Endpoint	Method	Description
/	GET	Main user interface
/ask	POST	Ask a question via HTML form
/api/ask	POST	API endpoint for JSON queries
/health	GET	Health check
/admin/dashboard	GET	Dashboard page
/admin/api/metrics	GET	Metrics API
/admin/api/clear-history	POST	Clear logs

ğŸ“¦ Example requirements.txt
nginx
Copy code
fastapi
uvicorn
jinja2
langchain
openai
faiss-cpu
python-multipart
pypdf
ğŸ§° Troubleshooting
Problem	Possible Solution
âŒ QA system not ready	Start LM Studio server with TinyLlama loaded
âš ï¸ ModuleNotFoundError: qa_engine	Check folder structure & __init__.py
ğŸ”’ Dashboard unauthorized	Provide correct admin token or update .env
ğŸ–¼ï¸ Static/Template not loading	Ensure static/ & templates/ paths are correct

ğŸ§‘â€ğŸ’» Tech Stack
Layer	Technology
Frontend	HTML + CSS + JS + Jinja2
Backend	FastAPI
AI Engine	TinyLlama (via LM Studio)
Framework	LangChain
Storage	FAISS (Vector Store)
Charts	Chart.js
UI Styling	Bootstrap 5

ğŸ“¸ Preview
User Interface:

Admin Dashboard:

ğŸš€ Future Enhancements
 ğŸ”„ Live answer streaming via WebSocket

 ğŸ’¾ Persistent metrics with SQLite

 ğŸ§­ Multi-PDF knowledge base

 ğŸŒ™ Dark mode for dashboard

 ğŸ¤– Support for other local models in LM Studio

