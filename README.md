# ğŸª´ Home Remedies AI - TinyLlama PDF Q&A System

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com/)
[![LangChain](https://img.shields.io/badge/LangChain-Latest-orange.svg)](https://langchain.com/)
[![TinyLlama](https://img.shields.io/badge/TinyLlama-1.1B-purple.svg)](https://huggingface.co/TinyLlama)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

An intelligent **AI-powered Question-Answering system** that extracts knowledge from PDF documents using **TinyLlama** language model and **LangChain** framework. Get instant answers about home remedies from your PDF knowledge base!

> ğŸ¤– **Powered by TinyLlama 1.1B** - A compact yet powerful language model running locally via LM Studio
> 
> ğŸ“š **RAG Architecture** - Retrieval-Augmented Generation for accurate, context-aware answers

---

## ğŸŒŸ Features

### ğŸ§  AI-Powered Intelligence
- **TinyLlama Language Model** - Efficient 1.1B parameter model
- **LM Studio Integration** - Run LLM locally without cloud APIs
- **Retrieval-Augmented Generation (RAG)** - Combines retrieval with generation
- **Semantic Search** - Find relevant information intelligently
- **Context-Aware Answers** - Answers grounded in your PDF content

### ğŸ“„ PDF Processing
- **Automatic PDF Loading** - Process any PDF document
- **Semantic Chunking** - Intelligent document splitting
- **Vector Embeddings** - HuggingFace sentence transformers
- **FAISS Vector Store** - Fast similarity search
- **Persistent Storage** - Cache vector store for faster loading

### ğŸ¨ Web Interface
- **FastAPI Backend** - High-performance async framework
- **Clean UI** - Simple, intuitive question-answering interface
- **Real-time Processing** - Instant answer generation
- **Source Citations** - View source documents used

### ğŸ”§ Modular Architecture
- **Separated Components** - PDF processor, vector store, QA chain
- **Easy Configuration** - Customize model and parameters
- **Extensible Design** - Add new features easily
- **Caching Support** - Optional vector store persistence

---

## ğŸ“¸ Screenshots

### Home Page
*Ask your health and home remedy questions*

### Results Page
*Get AI-powered answers with context*

---

## ğŸ“‹ Table of Contents

- [Quick Start](#-quick-start)
- [Installation](#-installation)
- [Project Structure](#-project-structure)
- [Usage Guide](#-usage-guide)
- [Architecture](#-architecture)
- [API Endpoints](#-api-endpoints)
- [Configuration](#-configuration)
- [Troubleshooting](#-troubleshooting)
- [Contributing](#-contributing)
- [License](#-license)

---

## ğŸš€ Quick Start

### ğŸ“‹ Prerequisites

Before you begin, ensure you have:

- âœ… **Python 3.8+** installed
- âœ… **pip** package manager
- âœ… **LM Studio** installed and running ([Download here](https://lmstudio.ai/))
- âœ… **TinyLlama model** loaded in LM Studio
- âœ… **Git** for cloning the repository
- âœ… **PDF document** (Basic_Home_Remedies.pdf or your own)

---

## âš¡ Installation

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/yourusername/Home_Remedies_AI_VibhavRaj.git
cd Home_Remedies_AI_VibhavRaj
```

### 2ï¸âƒ£ Create Virtual Environment

**Windows:**
```bash
python -m venv venv
venv\Scripts\activate
```

**Linux/Mac:**
```bash
python3 -m venv venv
source venv/bin/activate
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

**Core Dependencies:**
- `fastapi` - Web framework
- `uvicorn` - ASGI server
- `langchain` - LLM framework
- `langchain-community` - Community integrations
- `langchain-openai` - OpenAI-compatible API
- `sentence-transformers` - Embeddings
- `faiss-cpu` - Vector database
- `pdfplumber` - PDF processing
- `jinja2` - Template engine

### 4ï¸âƒ£ Setup LM Studio

1. **Download and Install** [LM Studio](https://lmstudio.ai/)
2. **Load TinyLlama Model:**
   - Open LM Studio
   - Search for "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
   - Download the model
   - Load the model in the server
3. **Start Local Server:**
   - Go to "Local Server" tab
   - Click "Start Server"
   - Default: `http://127.0.0.1:1234/v1`

### 5ï¸âƒ£ Add Your PDF Document

Place your PDF file in the root directory:
```bash
# Use provided file or add your own
Basic_Home_Remedies.pdf
```

### 6ï¸âƒ£ Run the Application

**Option A: FastAPI Web Interface**
```bash
uvicorn app:app --reload --host 127.0.0.1 --port 8000
```

**Option B: Command Line Interface**
```bash
python main.py
```

### 7ï¸âƒ£ Access the Application

**Web Interface:** Navigate to `http://127.0.0.1:8000`

**CLI:** Follow the prompts in your terminal

ğŸ‰ **Start asking questions about home remedies!**

---

## ğŸ“ Project Structure

```
Home_Remedies_AI_VibhavRaj/
â”‚
â”œâ”€â”€ app.py                              # FastAPI web application
â”œâ”€â”€ main.py                             # CLI interface
â”œâ”€â”€ Basic_Home_Remedies.pdf             # Knowledge base PDF
â”œâ”€â”€ requirements.txt                    # Python dependencies
â”œâ”€â”€ README.md                           # Project documentation
â”œâ”€â”€ .gitattributes                      # Git attributes
â”‚
â”œâ”€â”€ qa_engine/                          # Core QA engine modules
â”‚   â”œâ”€â”€ __init__.py                     # Main QA system interface
â”‚   â”œâ”€â”€ pdf_processor.py                # PDF loading and chunking
â”‚   â”œâ”€â”€ vector_store.py                 # FAISS vector store manager
â”‚   â””â”€â”€ qa_chain.py                     # LangChain QA chain
â”‚
â”œâ”€â”€ templates/                          # HTML templates
â”‚   â”œâ”€â”€ index.html                      # Home page (question form)
â”‚   â””â”€â”€ result.html                     # Answer display page
â”‚
â”œâ”€â”€ static/                             # Static assets
â”‚   â””â”€â”€ style.css                       # Custom styling
â”‚
â”œâ”€â”€ models/                             # Cached models (auto-generated)
â”‚   â””â”€â”€ faiss_index/                    # Persistent vector store
â”‚
â”œâ”€â”€ venv/                               # Virtual environment (gitignored)
â””â”€â”€ __pycache__/                        # Python cache (gitignored)
```

---

## ğŸ’» Usage Guide

### ğŸŒ Web Interface Usage

#### ğŸ”¹ Step 1: Start the Server

```bash
# Make sure LM Studio is running first!
uvicorn app:app --reload --host 127.0.0.1 --port 8000
```

#### ğŸ”¹ Step 2: Ask a Question

1. Open browser to `http://127.0.0.1:8000`
2. You'll see the **Home Remedies AI** interface
3. Type your question in the input box
4. Click "Ask" button

**Example Questions:**
- "What are natural remedies for headache?"
- "How to treat cold naturally?"
- "What herbs help with digestion?"
- "Natural remedies for sleep problems?"

#### ğŸ”¹ Step 3: View the Answer

- AI processes your question
- Retrieves relevant context from PDF
- Generates answer using TinyLlama
- Displays answer on results page
- Click "Ask another question" to continue

### ğŸ–¥ï¸ Command Line Usage

```bash
python main.py
```

**Interactive CLI:**
```
ğŸ¤– TinyLlama + LangChain PDF Q&A (Direct HuggingFace)
ğŸ  Initializing Home Remedies QA System
====================================================================
ğŸ“„ Loading PDF...
âœ… 50 pages loaded
ğŸ§© Split into 120 chunks
ğŸ“Š Creating vector store...
âœ… Vector store created
ğŸ¤– Loading tinyllama-1.1b-chat-v1.0...
âœ… LLM loaded
ğŸ”— Building QA chain...
âœ… QA chain created
====================================================================
âœ… System Ready! TinyLlama is loaded and ready to answer questions.
====================================================================

â“ Question: What helps with fever?

ğŸ§  Answer: Natural remedies for fever include drinking plenty of fluids, 
resting adequately, and using cool compresses. Herbal teas like ginger 
or chamomile can also help reduce temperature naturally.
```

---

## ğŸ—ï¸ Architecture

### ğŸ“ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      User Interface                          â”‚
â”‚              (FastAPI Web App / CLI)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   HomeRemedyQA System                        â”‚
â”‚              (qa_engine/__init__.py)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚              â”‚              â”‚
        â–¼              â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PDFProcessor â”‚ â”‚  Vector  â”‚ â”‚ QAChain     â”‚
â”‚              â”‚ â”‚  Store   â”‚ â”‚ Manager     â”‚
â”‚ - Load PDF   â”‚ â”‚ - FAISS  â”‚ â”‚ - TinyLlama â”‚
â”‚ - Chunk Text â”‚ â”‚ - Search â”‚ â”‚ - Prompt    â”‚
â”‚ - Embeddings â”‚ â”‚ - Cache  â”‚ â”‚ - Generate  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚              â”‚              â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  LM Studio      â”‚
              â”‚  (TinyLlama)    â”‚
              â”‚  Local Server   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”„ RAG Pipeline

```
1. Question Input
         â†“
2. Embedding Generation (HuggingFace)
         â†“
3. Semantic Search (FAISS)
         â†“
4. Context Retrieval (Top K Documents)
         â†“
5. Prompt Construction (Question + Context)
         â†“
6. LLM Generation (TinyLlama via LM Studio)
         â†“
7. Answer Output (With Source Citations)
```

### ğŸ§© Component Details

#### ğŸ“„ **PDFProcessor** (`pdf_processor.py`)
- Loads PDF using PDFPlumber
- Extracts text content
- Creates semantic chunks using SemanticChunker
- Generates embeddings with HuggingFace transformers

#### ğŸ—„ï¸ **VectorStoreManager** (`vector_store.py`)
- Creates FAISS vector index
- Stores document embeddings
- Performs similarity search
- Supports persistence (save/load)

#### ğŸ¤– **QAChainManager** (`qa_chain.py`)
- Connects to LM Studio API
- Creates LangChain RetrievalQA chain
- Manages prompts and generation
- Returns answers with sources

#### ğŸ  **HomeRemedyQA** (`__init__.py`)
- Orchestrates all components
- Provides simple API interface
- Handles initialization
- Manages caching

---

## ğŸ”Œ API Endpoints

### Web Application Routes

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/` | Main page (index.html) |
| `POST` | `/ask` | Process question and generate answer |
| `GET` | `/result` | Display answer (result.html) |

### Request Examples

**Ask a Question:**
```bash
curl -X POST "http://127.0.0.1:8000/ask" \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "question=What helps with headache?"
```

**Response Format:**
The result is rendered in `result.html` with:
```html
Question: {{ question }}
Answer: {{ answer }}
```

---

## âš™ï¸ Configuration

### ğŸ”§ Environment Variables

Create a `.env` file (optional):

```env
# LM Studio Configuration
OPENAI_API_BASE=http://127.0.0.1:1234/v1
OPENAI_API_KEY=lm-studio

# Model Settings
MODEL_NAME=tinyllama-1.1b-chat-v1.0
TEMPERATURE=0.7
REQUEST_TIMEOUT=60

# Embedding Settings
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Vector Store Settings
VECTOR_STORE_PATH=models/faiss_index
USE_CACHE=True

# Retrieval Settings
SEARCH_TYPE=similarity
TOP_K_RESULTS=3
```

### ğŸ› ï¸ Customizing the QA Engine

Edit `qa_engine/__init__.py`:

```python
# Initialize with custom settings
qa_system = HomeRemedyQA(
    pdf_path="your_document.pdf",
    use_cache=True  # Enable vector store caching
)

# Get relevant documents without asking
docs = qa_system.get_relevant_docs("your question", k=5)
```

Edit `qa_engine/qa_chain.py`:

```python
# Customize prompt template
prompt_template = """You are a helpful assistant.
Use the context below to answer the question.

Context:
{context}

Question:
{question}

Answer in a friendly, conversational tone:"""
```

### ğŸ“Š PDF Processing Settings

Edit `qa_engine/pdf_processor.py`:

```python
# Change embedding model
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-mpnet-base-v2"  # More powerful
)

# Adjust chunk size
splitter = SemanticChunker(
    embeddings,
    breakpoint_threshold_amount=0.5  # Controls chunk boundaries
)
```

### ğŸ—„ï¸ Vector Store Configuration

Edit `qa_engine/vector_store.py`:

```python
# Adjust retrieval parameters
retriever = vector_store.as_retriever(
    search_type="similarity",
    search_kwargs={
        "k": 5,  # Return top 5 results
        "fetch_k": 20,  # Fetch 20 candidates first
        "score_threshold": 0.7  # Minimum similarity score
    }
)
```

---

## ğŸ› Troubleshooting

### âŒ Common Issues & Solutions

**Issue 1: LM Studio Connection Error**
```bash
# Error: "Connection refused to http://127.0.0.1:1234/v1"
# Solution:
1. Open LM Studio
2. Load TinyLlama model
3. Go to "Local Server" tab
4. Click "Start Server"
5. Verify server is running at http://127.0.0.1:1234
```

**Issue 2: Module Not Found**
```bash
# Error: "ModuleNotFoundError: No module named 'langchain'"
# Solution:
pip install -r requirements.txt --upgrade
pip install langchain langchain-community langchain-openai
```

**Issue 3: PDF Not Loading**
```bash
# Error: "FileNotFoundError: Basic_Home_Remedies.pdf"
# Solution:
# Ensure PDF is in root directory
ls Basic_Home_Remedies.pdf  # Check file exists

# Or specify custom path
qa_system = HomeRemedyQA(pdf_path="path/to/your/document.pdf")
```

**Issue 4: FAISS Installation Error (Windows)**
```bash
# For Windows users
pip install faiss-cpu
# If that fails, try:
conda install -c pytorch faiss-cpu
```

**Issue 5: Slow Response Times**
```bash
# Solution 1: Enable caching
qa_system = HomeRemedyQA(use_cache=True)

# Solution 2: Reduce retrieved documents
retriever = vector_store.as_retriever(search_kwargs={"k": 2})

# Solution 3: Use smaller embedding model
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)
```

**Issue 6: Template Not Found**
```bash
# Error: "TemplateNotFound: index.html"
# Solution: Verify templates folder structure
templates/
â”œâ”€â”€ index.html
â””â”€â”€ result.html
```

**Issue 7: Out of Memory**
```bash
# Solution: Process PDF in smaller chunks
# Or use a smaller model in LM Studio
# Or reduce the number of retrieved documents
```

---

## ğŸ“¦ Requirements

**Core Dependencies:**
```txt
fastapi==0.104.1
uvicorn[standard]==0.24.0
langchain==0.1.0
langchain-community==0.0.10
langchain-openai==0.0.2
sentence-transformers==2.2.2
faiss-cpu==1.7.4
pdfplumber==0.10.3
jinja2==3.1.2
python-multipart==0.0.6
```

**Optional Dependencies:**
```txt
python-dotenv==1.0.0    # For environment variables
langchain-experimental==0.0.47  # For advanced features
```

---

## ğŸ¤ Contributing

We welcome contributions to improve the Home Remedies AI system!

### ğŸ”„ How to Contribute

1. **Fork** the repository
2. Create a **feature branch**: `git checkout -b feature/AmazingFeature`
3. **Commit** changes: `git commit -m "âœ¨ Add: New feature"`
4. **Push** to branch: `git push origin feature/AmazingFeature`
5. Open a **Pull Request**

### ğŸ“ Contribution Guidelines

- Follow PEP 8 style guide
- Add docstrings to functions
- Include type hints where possible
- Write clear commit messages
- Update README for significant changes
- Test your changes thoroughly

### ğŸ¯ Areas for Contribution

- [ ] Add support for multiple PDF documents
- [ ] Implement conversation history/memory
- [ ] Add source document highlighting
- [ ] Create API documentation (Swagger)
- [ ] Add unit tests
- [ ] Improve UI/UX design
- [ ] Add support for other document formats (DOCX, TXT)
- [ ] Implement streaming responses
- [ ] Add authentication system
- [ ] Create Docker containerization

---

## ğŸš€ Future Enhancements

### ğŸ¯ Planned Features

#### ğŸ§  AI Improvements
- [ ] Support for larger language models (Llama 2, Mistral)
- [ ] Multi-turn conversation with memory
- [ ] Hybrid search (semantic + keyword)
- [ ] Query expansion and reformulation
- [ ] Answer confidence scoring

#### ğŸ“š Document Processing
- [ ] Multi-document support
- [ ] Document upload via web interface
- [ ] Support for DOCX, TXT, HTML formats
- [ ] Automatic document categorization
- [ ] OCR for scanned PDFs

#### ğŸ¨ User Interface
- [ ] Modern, responsive design
- [ ] Dark mode toggle
- [ ] Real-time typing animation
- [ ] Source document viewer
- [ ] Search history
- [ ] Bookmark favorite answers

#### ğŸ”§ Technical Features
- [ ] REST API with authentication
- [ ] WebSocket for streaming responses
- [ ] Redis caching layer
- [ ] PostgreSQL for conversation history
- [ ] Docker deployment
- [ ] Kubernetes orchestration

#### ğŸ“Š Analytics
- [ ] Question analytics dashboard
- [ ] Most asked questions
- [ ] User feedback collection
- [ ] Answer quality metrics

---

## ğŸ‘¨â€ğŸ’» Author

**Vibhav Raj**  
ğŸ¤– AI/ML Engineer | ğŸ¥ Healthcare Technology Enthusiast

[![GitHub](https://img.shields.io/badge/GitHub-Follow-black?style=social&logo=github)](https://github.com/vibhavraj)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue?style=social&logo=linkedin)](https://linkedin.com/in/vibhavraj)


### ğŸ’¼ About Me
- ğŸ“ Passionate about AI applications in healthcare
- ğŸ’» Specializing in NLP and LLM applications
- ğŸŒŸ Building practical AI solutions for real-world problems
- ğŸ“š Contributing to open-source AI projects

---

## ğŸ™ Acknowledgments

Special thanks to:

- **[LangChain](https://langchain.com/)** - For the powerful LLM framework
- **[TinyLlama Team](https://huggingface.co/TinyLlama)** - For the efficient language model
- **[LM Studio](https://lmstudio.ai/)** - For easy local LLM deployment
- **[HuggingFace](https://huggingface.co/)** - For embeddings and model hosting
- **[FastAPI Team](https://fastapi.tiangolo.com/)** - For the modern web framework
- **[FAISS](https://github.com/facebookresearch/faiss)** - For efficient similarity search
- **Open Source Community** - For tools and inspiration

---

## ğŸ“š Resources & References

### ğŸ“– Documentation
- [LangChain Documentation](https://python.langchain.com/)
- [TinyLlama Model Card](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [FAISS Documentation](https://faiss.ai/)
- [LM Studio Guide](https://lmstudio.ai/docs)

### ğŸ”¬ Research Papers
- "TinyLlama: An Open-Source Small Language Model"
- "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"
- "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"

### ğŸ“ Tutorials
- [Building a RAG System with LangChain](https://python.langchain.com/docs/use_cases/question_answering/)
- [FAISS Vector Database Tutorial](https://www.pinecone.io/learn/faiss/)
- [FastAPI for Beginners](https://fastapi.tiangolo.com/tutorial/)

---

## ğŸ“ Support & Contact

### ğŸ› Found a Bug?
Open an issue: [GitHub Issues](https://github.com/vibhavraj/Home_Remedies_AI_VibhavRaj/issues)

### ğŸ’¬ Need Help?
- ğŸ“§ Email: vibhavraj@example.com
- ğŸ’¬ Discord: [Join our AI community](#)
- ğŸ¦ Twitter: [@vibhavraj](#)
- ğŸ’¼ LinkedIn: [Vibhav Raj](#)

### ğŸ’° Support the Project
If this project helped you:

[![Buy Me A Coffee](https://img.shields.io/badge/Buy%20Me%20A%20Coffee-Support-yellow?style=for-the-badge&logo=buy-me-a-coffee)](https://buymeacoffee.com/vibhavraj)
[![Sponsor](https://img.shields.io/badge/Sponsor-GitHub-pink?style=for-the-badge&logo=github)](https://github.com/sponsors/vibhavraj)

---

## ğŸ“œ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

### ğŸ“„ MIT License Summary
- âœ… Commercial use
- âœ… Modification
- âœ… Distribution
- âœ… Private use
- âŒ Liability
- âŒ Warranty

---

## ğŸ“Š Project Stats

![GitHub stars](https://img.shields.io/github/stars/vibhavraj/Home_Remedies_AI_VibhavRaj?style=social)
![GitHub forks](https://img.shields.io/github/forks/vibhavraj/Home_Remedies_AI_VibhavRaj?style=social)
![GitHub watchers](https://img.shields.io/github/watchers/vibhavraj/Home_Remedies_AI_VibhavRaj?style=social)
![GitHub issues](https://img.shields.io/github/issues/vibhavraj/Home_Remedies_AI_VibhavRaj)
![GitHub last commit](https://img.shields.io/github/last-commit/vibhavraj/Home_Remedies_AI_VibhavRaj)

---

## â­ Show Your Support

If you found this project useful, please consider:

1. â­ **Star** this repository
2. ğŸ”€ **Fork** for your own experiments
3. ğŸ“¢ **Share** with the AI community
4. ğŸ› **Report** issues to help improve
5. ğŸ’¡ **Contribute** new features

---

<div align="center">

### ğŸª´ Powered by AI | Built with â¤ï¸ and Python

**Your Personal Health Assistant - Always Ready to Help** ğŸ¥

[Documentation](#) | [Demo](#) | [Community](#) | [Blog](#)

---

*Bridging traditional wisdom with modern AI technology*

</div>
